{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline using Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import copy\n",
    "pos_tagger = nltk.data.load(nltk.tag._POS_TAGGER)\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import operator\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tag.stanford import NERTagger\n",
    "from collections import defaultdict\n",
    "import nltk.data\n",
    "java_path = \"C:/Program Files/Java/jdk1.7.0_71/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "tag_file1 = '.\\\\Others\\\\stanford_ner\\\\ner\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz'\n",
    "tag_file2 = '.\\\\Others\\\\stanford_ner\\\\ner\\\\stanford-ner.jar'\n",
    "ner_tagger = NERTagger(tag_file1,tag_file2,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"Others\\Features.png\" alt=\"HTML5 Icon\" width=\"800\" height=\"500\", style=\"display: ;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root_dir = '..\\data\\DUC2001'\n",
    "annotation_file = 'annotations.txt'\n",
    "txt_opn_tag = '<TEXT>'\n",
    "txt_close_tag = '</TEXT>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_and_its_files(data_root_dir,annotation_file):\n",
    "    '''Get a Cluster and the file names associated with it\n",
    "       Returns a dictionary of the form { cluster_1 : [file1,file2,file3....], cluster_2 : [file1,file2,file3....] }'''    \n",
    "    \n",
    "    f = open(data_root_dir + '\\\\' + annotation_file,'r')\n",
    "    \n",
    "    clust_files = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for line in f.readlines():\n",
    "        cur_line = line.split(';')[0]\n",
    "        clust_name = cur_line.split('@')[1]\n",
    "        file_name = cur_line.split('@')[0]\n",
    "        \n",
    "        clust_files[clust_name].append(file_name)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return clust_files\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP900322-0200', 'FBIS-41815', 'FBIS-45908', 'FT921-9310', 'FT931-3883', 'FT933-8272', 'FT941-575', 'LA042290-0104', 'LA060490-0083', 'WSJ910107-0139']\n"
     ]
    }
   ],
   "source": [
    "print get_cluster_and_its_files(data_root_dir,annotation_file)['mad cow disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_doc(document_path,txt_opn_tag,txt_close_tag):\n",
    "    \n",
    "    f = open(document_path,'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    start = content.index(txt_opn_tag) + len(txt_opn_tag)\n",
    "    end   = content.index(txt_close_tag)\n",
    "    \n",
    "    return content[start:end]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n   Millions of gallons of crude oil that\\nspilled when a tanker ran aground spread across a wildlife-rich\\nstretch of ocean Saturday, and Alaska's chief environmental officer\\ncriticized cleanup efforts as too slow.\\n   The biggest oil spill in U.S. history created a slick about\\nseven miles long and seven miles wide in Prince William Sound. The\\nCoast Guard said only Reef Island and the western edge of Bligh\\nIsland had been touched by the slick.\\n   ``This situation, I think, was everyone's secret nightmare about\\nwhat could happen with oil traffic in the sound,'' said Dennis\\nKelso, commissioner of the Alaska Department of Environmental\\nConservation.\\n   Some 240,000 barrels _ about 10,080,000 gallons _ of crude oil\\nfrom Alaska's North Slope spilled early Friday when the 987-foot\\ntanker Exxon Valdez ran hard aground on Bligh Reef, about 25 miles\\noutside Valdez, where it had taken on a total cargo of 1.2 million\\nbarrels. Initial reports indicated 270,000 barrels had spilled.\\n   ``What we have here is a major environmental catastrophe,'' said\\none oil spill expert, Richard Golob of Boston, publisher of Golob\\nOil Pollution Bulletin.\\n   Golob said cleanup equipment at the site was ``grossly\\ninadequate'' but added that even under ideal circumstances cleanup\\nefforts would not have significantly reduced the spill's impact.\\n   ``It is an enclosed body of water,'' he said. ``The only way for\\nthis oil to ecape out to the sea is by traversing the entire length\\nof Prince William Sound with all its islands, fjords and bays and\\nchannels.\\n   ``And during that transit, undoubtedly a large stretch of\\nshoreline will be contaminated,'' he said.\\n   Divers Saturday said they had found six to eight holes in the\\nvessel's hull large enough to swim through, said Frank Iarossi,\\npresident of Exxon Shipping Co. About 30 feet of the vessel is\\nresting on a shelf on the reef.\\n   Efforts to begin pumping 200,000 gallons of oil off the Exxon\\nValdez onto another tanker, the Exxon Baton Rouge, were halted\\nearly Saturday when authorities noticed that oil appeared to\\nleaking as the pumping operation proceeded.\\n   Eleven of 17 tanks that lie forward of the ship's masthead were\\nruptured in the accident, causing concern over removal of the oil,\\nsaid Coast Guard Lt. Ed Wieliczkiewicz.\\n   ``Whenever you start removing oil from a vessel this size it has\\nto be done in a controlled manner,'' Wieliczkiewicz said. ``If it's\\nnot ... you endanger the stability of the vessel.''\\n   Wieliczkiewicz said a boom was placed around the Exxon Valdez\\nand the Exxon Baton Rouge to help contain oil around the vessels.\\n   He also said four members of the Coast Guard's Pacific Strike\\nTeam from San Francisco, specially trained to deal with pollution\\nand oil spills, arrived Saturday and were helping to rig pumps and\\nassemble equipment needed to transfer oil to the Baton Rouge.\\n   Kelso was highly critical of what he said was a slow response to\\nthe spill.\\n   ``The initial reponse was inadequate and unacceptable,'' he said\\nbefore a news conference Saturday. Kelso said the efforts should\\nhave been under way in five hours, but took much longer. ``You miss\\nthe opportunity right at the beginning and you've missed our best\\nopportunity to do something.''\\n   Kelso said Alaska has a plan for oil spills that calls for\\naction within five hours of a spill. It took several hours longer,\\nhe said, and only two of seven skimmers available to the Alyeska\\nPipeline Service Co. were used at the outset.\\n   Alyeska spokesman Chuck O'Donnel said he was satisfied with his\\ncompany's actions. ``I think our people did an excellent job,'' he\\nsaid.\\n   The spill's effect on wildlife had not yet been assessed, but\\ncommercial fishermen who depend on the sound for a catch worth\\nmillions of dollars were outraged and said a key herring spawning\\narea had been polluted.\\n   ``The whole food chain could be affected by the spill,'' said\\nAlan Reichman, ocean ecology coordinator for the environmental\\ngroup Greenpeace, in Seattle.\\n   ``There's a high concentration of sea otter, waterfowl, sea\\nbirds and pink salmon in that area,'' said Steve Goldstein, a\\nspokesman for the Interior Department in Washington. ``Some birds\\nhave already died, and we are doing our best to try to save the\\nfish by containing the oil to the area where it presently is and by\\ntrying to skim it.''\\n   Whales, porpoises and seals are also common in Prince William\\nSound. ``It's kind of like sailing through a zoo,'' said Jim\\nLethcoe, who lives on a boat in the sound and operates a sailing\\nbusiness.\\n   An animal cleanup station was set up in a building at the\\ncommunity college in Valdez, but volunteers there said they had no\\nanimals to work on by midafternoon.\\n   The response to the spill also drew fire from the 12,000-member\\nUnited Fishermen of Alaska.\\n   ``We feel that this should have been the easiest oil spill in\\nthe world to clean up,'' said Riki Ott, chairman of the\\norganization's habitat committee. She noted that the spill had\\noccurred in a protected area close to the Valdez marine terminal\\nand the water was calm.\\n   Ott said the spill had polluted Prince William Sound's primary\\nherring spawning area. Fishermen also take salmon and shellfish\\nfrom the sound. Last year, they were paid about $85 million for\\ntheir catches, she said.\\n   The Port of Valdez remained closed to tanker traffic. North\\nSlope crude oil is shipped 800 miles through the trans-Alaska oil\\npipeline from Prudhoe Bay south to Valdez for shipment aboard\\ntankers to refineries outside Alaska.\\n   The Coast Guard said the Exxon Valdez struck the reef when it\\nmaneuvered outside normal tanker traffic lanes to avoid icebergs.\\n   The vessel's captain, Joseph Hazelwood, has worked for Exxon for\\n20 years, at least 10 as a ship's master. It was unclear if a pilot\\nwas aboard the Exxon Valdez when it grounded.\\n   There was no decision Saturday on whether to use chemicals to\\ndisperse the oil, but a test of the dispersal method was being\\nconducted Saturday afternoon, as was a test to determine whether at\\nleast some of the crude oil could be burned.\\n   Coast Guard Cmdr. Steven McCall said National Transportation\\nSafety Board investigators are expected to arrive Sunday to take\\nover the accident probe.\\n   He said one or more blood-alcohol tests were administered after\\nthe grounding, but he said he didn't know know how many people were\\ntested or the results. McCall said the tests routinely are\\nadministered in marine accidents involving federal jurisdiction.\\n   The Coast Guard issued a statement late Saturday that McCall has\\nsubpoenaed the ship's master and two crew members.\\n   The subpoenas require them to make themselves available to NTSB\\ninvestigators arriving Sunday. The Coast Guard said the supoenas\\nwere routine.\\n   Previously, the largest U.S. tanker spill was the Dec. 15, 1976,\\ngrounding of the Argo Merchant tanker off the Nantucket shoals off\\nMassachusetts, in which 7.6 million gallons of oil spilled, Golob\\nsaid.\\n   Up to 10.7 million gallons of oil was lost on Nov. 1, 1979, when\\nthe tanker Burmah Agate collided with another ship in Galveston\\nBay, Texas. However, that oil burned as well as spilled.\\n   The largest tanker spill in history resulted from the July 19,\\n1979, collision off Tobago of the supertankers Atlantic Empress and\\nAegean Captain, in which 300,000 tons _ more than 80 million\\ngallons _ of oil was lost.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'AP830325-0143'\n",
    "file_path = data_root_dir + '\\\\' + file_name\n",
    "get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_txt(text,nltk_flag=True,ner_flag=False):\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if ner_flag == True:        \n",
    "        tokenizedList = re.split('[^a-zA-Z]+', text.lower())\n",
    "        return tokenizedList\n",
    "    \n",
    "    if nltk_flag == False:\n",
    "        #return [x.lower() for x in re.findall(r\"\\w+\", text)]\n",
    "\n",
    "        tokenizedList = re.split('\\W+', text.lower())\n",
    "        return [unicode(x,'utf-8') for x in tokenizedList if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_']\n",
    "    else:\n",
    "        return nltk.word_tokenize(unicode(text,'utf-8')) \n",
    "        #return [x for x in toks if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_' and x!= ',' and x != '.']    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'this', 'is', 'this', 'cool', 'i', 'don', 't', 'know']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_txt('What is this ?? Is this _ cool ? I don\\'t know',nltk_flag=True,ner_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 1 : Term frequency over the cluster(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_term_freqs(data_root_dir,annotation_file,stop_words=None) :\n",
    "    '''Get the term freqs of words in clusters. The term freqs are unique to clusters.\n",
    "    Returns a dict of form {clust1 : {word1 : 2, word2 :3...},clust2 : {word1 : 2, word2 :3..} ......}'''\n",
    "        \n",
    "    #Check about stop_words\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_term_freq = defaultdict(defaultdict)\n",
    "    \n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        term_freq = defaultdict(int)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                term_freq[token] += 1\n",
    "        \n",
    "        clust_term_freq[clust] = term_freq\n",
    "    \n",
    "    return clust_term_freq\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {u'all': 1, u'Union': 1, u'Kretzschmar': 1, u'Switzerland': 1, u'per': 1, u'human': 1, u'still': 1, u'decisions': 1, u'its': 1, u'European': 1, u'Jakob': 1, u'one': 1, u'March': 1, u'(': 2, u'had': 2, u',': 10, u'should': 1, u'to': 6, u'safeguards': 1, u'do': 1, u'popularly': 1, u'affected': 1, u'diseases': 1, u'than': 1, u'government': 1, u'very': 1, u'100,000': 1, u'scientists': 1, u'possible': 1, u'Gottingen': 1, u'were': 3, u'know': 1, u'not': 3, u'affect': 2, u'existing': 1, u'countries': 1, u'medicines': 1, u'50': 1, u'whether': 1, u'transmitted': 2, u'minimal': 1, u'ban': 2, u'Contaminated': 1, u'because': 1, u'humans': 4, u'bovine': 1, u'connections': 1, u'likely': 1, u'catching': 1, u'are': 1, u'encephalopathy': 1, u'further': 1, u'institutes': 1, u'agriculture': 1, u'concern': 1, u'universities': 1, u'project': 1, u'said': 3, u'imported': 3, u'for': 2, u'1992': 1, u'recorded': 1, u'expressed': 1, u'research': 4, u'I': 1, u'health': 1, u'between': 1, u'new': 1, u'contaminated': 2, u'University': 1, u'announced': 1, u'available': 1, u'be': 7, u'we': 1, u'Professor': 1, u'pushing': 1, u'EU': 2, u'by': 2, u'official': 1, u'on': 2, u'about': 1, u'last': 1, u'would': 1, u'origins': 1, u'launch': 1, u'of': 11, u'30': 1, u'discussed': 1, u'figures': 1, u'argue': 1, u'or': 2, u'comes': 1, u\"'The\": 1, u'Gerstmann': 1, u'danger': 1, u'year': 1, u'ministers': 2, u'beings': 1, u'initiative': 1, u'been': 1, u'rarely': 1, u'from': 4, u'beef': 6, u'debilitates': 1, u'union': 1, u'there': 2, u'two': 2, u'cent': 1, u'.': 11, u'2': 1, u'way': 1, u'Hans': 1, u'BSE': 5, u'meeting': 1, u'more': 1, u'-may': 1, u'spongiform': 1, u'that': 8, u'brains': 1, u'sufficient': 1, u'but': 1, u'personally': 1, u'known': 2, u'cases': 2, u'with': 1, u'eat': 1, u'2,092': 1, u'Seven': 1, u'made': 2, u'animals': 1, u'cow': 2, u'transmissible': 1, u'German': 4, u'ministry': 2, u'as': 3, u'will': 2, u'Britain': 2, u'can': 3, u'country': 2, u'Straussler': 1, u'at': 1, u'and': 6, u'non-existent': 2, u'imports': 2, u'is': 4, u'cattle': 3, u'it': 2, u'evidence': 1, u'examine': 2, u'ingredients': 1, u'have': 1, u'in': 3, u'Several': 1, u'technology': 1, u'any': 1, u'result': 1, u\"'mad\": 1, u'syndrome': 1, u'no': 1, u')': 2, u'-': 3, u'other': 2, u'Germany': 2, u'take': 1, u'which': 3, u'A': 1, u\"'s\": 2, u'arguing': 1, u'who': 2, u'yesterday': 1, u'British': 4, u'sponsored': 1, u'The': 3, u'13': 1, u'died': 1, u'conclusive': 1, u'a': 7, u\"'\": 3, u\"'However\": 1, u'Creutzfeldt': 1, u'disease': 5, u'think': 1, u'veal': 1, u'In': 1, u'tonnes': 2, u'the': 11})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_term_freqs(data_root_dir,annotation_file)['cattle disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature 2 : Total document number in the datasets, divided by the frequency of documents which contains this word (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_freqs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form {word1 : df1 , word2 : df2 ...}'''\n",
    "    '''Example : {furazabol : 154.5 , the : 1.00032}'''\n",
    "    \n",
    "    data_root_dir += '\\\\'\n",
    "    \n",
    "    docs =  [file_name for _,__,file_name in os.walk(data_root_dir)][0]\n",
    "    \n",
    "    if annotation_file in docs:\n",
    "        docs.remove(annotation_file)        \n",
    "    \n",
    "    inverted_index  = defaultdict(set)\n",
    "    \n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_path = data_root_dir + doc        \n",
    "        txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "        doc_tokens = tokenize_txt(txt)\n",
    "        \n",
    "        for token in doc_tokens:\n",
    "            inverted_index[token].add(doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    no_of_docs = len(docs)\n",
    "    idf_dict = defaultdict(float)\n",
    "    \n",
    "    for term,doc_lst in inverted_index.iteritems():\n",
    "        idf_dict[term] = float(no_of_docs) / len(doc_lst)\n",
    "    \n",
    "    return idf_dict\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.5\n",
      "1.00324675325\n"
     ]
    }
   ],
   "source": [
    "doc_freqs = get_doc_freqs(data_root_dir,annotation_file)\n",
    "print doc_freqs['furazabol']\n",
    "print doc_freqs['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 3 : The frequency of documents which contains this word in the current cluster (CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clusterwise_dfs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form : {clust1 : (word1 : df1,word2 :df2 .....) , clust1 : (word3 : df3,word2 :df3 .....)}'''\n",
    "    '''Note that the document frequencies of term are calculated clusterwise, and not on the whole dataset'''\n",
    "    \n",
    "    clust_doc_freqs = defaultdict(defaultdict)\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        inverted_index  = defaultdict(set)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                inverted_index[token].add(doc)\n",
    "        \n",
    "        \n",
    "        clust_df = defaultdict(int)\n",
    "        \n",
    "        for term,doc_lst in inverted_index.iteritems():\n",
    "            clust_df[term] =  len(doc_lst)\n",
    "        \n",
    "        clust_doc_freqs[clust] = clust_df\n",
    "    \n",
    "    return clust_doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 10), (u'encephalopathy', 10), (u'cow', 10), (u'and', 10), (u'.', 10), (u'in', 10), (u'the', 10), (u'has', 10), (u'for', 10), (u\"'s\", 10), (u'that', 10), (u'were', 10), (u'spongiform', 10), (u'of', 10), (u'with', 10), (u'as', 10), (u'to', 10), (u'a', 10), (u'be', 10), (u'from', 10)]\n"
     ]
    }
   ],
   "source": [
    "clust_dfs = get_clusterwise_dfs(data_root_dir,annotation_file)\n",
    "print sorted(clust_dfs['mad cow disease'].items(),key=operator.itemgetter(1),reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 4 : A 4-dimension binary vector indicates whether the word is a noun, a verb, an adjective or an adverb. If the word has\n",
    "another part-of-speech, the vector is all-zero  (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_short_tag(long_tag,valid_pos=['NN','VB','JJ','RB']):      \n",
    "    '''Truncate long_tag to get its first 2 chars. If a valid POS, return first 2 chars. else return OT (Other)'''\n",
    "    '''Valid POS are NN,VB,JJ,RB'''\n",
    "    \n",
    "    valid_pos_lst = valid_pos\n",
    "       \n",
    "    long_tag = str.upper(long_tag[0:2])\n",
    "    \n",
    "    if long_tag in valid_pos_lst:\n",
    "        return long_tag\n",
    "    \n",
    "    else:\n",
    "        return 'OT'                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentence_tags(sentence):\n",
    "    '''POS tag the words in the sentence and return a dict of the form : {word1 : [tag1,tag2..], word2 : [tag3,tag4..]..}'''\n",
    "    word_tag_dict = defaultdict(set)\n",
    "    sent_tags = pos_tagger.tag(tokenize_txt(sentence))\n",
    "        \n",
    "    for word_tag in sent_tags:\n",
    "        word = word_tag[0]\n",
    "        tag = word_tag[1]\n",
    "        \n",
    "        word_tag_dict[word].add(get_short_tag(tag))\n",
    "    \n",
    "    return word_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {u'sent': set(['NN']), u'one': set(['OT'])})\n",
      "defaultdict(<type 'set'>, {u'two': set(['OT']), u'sent': set(['NN'])})\n"
     ]
    }
   ],
   "source": [
    "print get_sentence_tags(\"sent one\")\n",
    "print get_sentence_tags(\"sent two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_tags(document):\n",
    "    \n",
    "    '''Perform POS tagging on all the sentences in the document and return a dict of the form :'''\n",
    "    ''' (sent_id : { word1 : tag1 ...}...}'''\n",
    "    \n",
    "    sent_and_tags = defaultdict(int)\n",
    "    \n",
    "    #sentences = document.split('.')\n",
    "    sentences = sent_detector.tokenize(document,realign_boundaries=True)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sent_and_tags[i] = get_sentence_tags(sentence.strip('.').strip('\\n'.strip('')))\n",
    "    \n",
    "    return sent_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: defaultdict(<type 'set'>, {u'be': set(['VB']), u'Trump': set(['NN']), u'would': set(['OT']), u'that': set(['OT']), u'apparent': set(['JJ']), u'it': set(['OT']), u'next': set(['JJ']), u'Donald': set(['NN']), u'became': set(['VB']), u'president': set(['NN']), u'the': set(['OT']), u'Today': set(['NN'])}), 1: defaultdict(<type 'set'>, {u'Clinton': set(['NN']), u'wouldnt': set(['NN']), u'However': set(['RB']), u',': set(['OT']), u'Hillary': set(['NN']), u'agree': set(['VB'])})})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_tags(\"Who is Alan Turing ??. Alan was born in the United Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_tags(data_root_dir,annotation_file):\n",
    "    '''Perfom Part of Speech Tagging across all the sentences in all the documents in all the clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_tags = defaultdict(defaultdict)\n",
    "    \n",
    "    i = 1\n",
    "    for clust,files in clust_files.iteritems():        \n",
    "        \n",
    "        for doc in files:\n",
    "            \n",
    "            if i %10 == 0:\n",
    "                print 'Finished tagging doc :', i\n",
    "            i += 1\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            \n",
    "            clust_tags[clust][doc] = get_doc_tags(txt)\n",
    "            \n",
    "    return clust_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tagging doc : 10\n",
      "Finished tagging doc : 20\n",
      "Finished tagging doc : 30\n",
      "Finished tagging doc : 40\n",
      "Finished tagging doc : 50\n",
      "Finished tagging doc : 60\n",
      "Finished tagging doc : 70\n",
      "Finished tagging doc : 80\n",
      "Finished tagging doc : 90\n",
      "Finished tagging doc : 100\n",
      "Finished tagging doc : 110\n",
      "Finished tagging doc : 120\n",
      "Finished tagging doc : 130\n",
      "Finished tagging doc : 140\n",
      "Finished tagging doc : 150\n",
      "Finished tagging doc : 160\n",
      "Finished tagging doc : 170\n",
      "Finished tagging doc : 180\n",
      "Finished tagging doc : 190\n",
      "Finished tagging doc : 200\n",
      "Finished tagging doc : 210\n",
      "Finished tagging doc : 220\n",
      "Finished tagging doc : 230\n",
      "Finished tagging doc : 240\n",
      "Finished tagging doc : 250\n",
      "Finished tagging doc : 260\n",
      "Finished tagging doc : 270\n",
      "Finished tagging doc : 280\n",
      "Finished tagging doc : 290\n",
      "Finished tagging doc : 300\n"
     ]
    }
   ],
   "source": [
    "clust_tags = get_cluster_tags(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serialize(file_name,data):\n",
    "    \n",
    "    with open(file_name, 'wb') as f:    \n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deserialize(file_name):\n",
    "\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'pos_tags.pickle'\n",
    "#serialize(file_name,clust_tags)\n",
    "clust_tags = deserialize(file_name)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cpy = copy.deepcopy(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_pos(pos_set,pos_idx = {'NN' : 0 ,'VB' : 1,'JJ' : 2,'RB' : 3}):\n",
    "    \n",
    "    '''Convert the POS set to a binary vector according to pos_idx'''    \n",
    "    bin_pos_vec = 4*[False]\n",
    "    \n",
    "    for pos in pos_set:\n",
    "        \n",
    "        if pos == 'OT':\n",
    "            return 4*[False]\n",
    "        else:\n",
    "            bin_pos_vec[pos_idx[pos]] = True\n",
    "    \n",
    "    return bin_pos_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, True]\n",
      "[False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print vectorize_pos({'NN','RB'})\n",
    "print vectorize_pos({'NN','RB','JJ','VB','OT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_tags_across_clusters(clust_tags):\n",
    "    '''Binarize the POS of words'''\n",
    "\n",
    "    for clust,doc in clust_tags.iteritems(): \n",
    "\n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "\n",
    "        for doc,sent in doc.iteritems():\n",
    "\n",
    "            sent_word = defaultdict(defaultdict)\n",
    "\n",
    "            for sen_id,word_pos in sent.iteritems():\n",
    "\n",
    "\n",
    "                for word,pos in word_pos.iteritems():                            \n",
    "                    word_pos[word] = copy.deepcopy(vectorize_pos(pos))\n",
    "\n",
    "                sent_word[sen_id] = copy.deepcopy(word_pos)\n",
    "\n",
    "            doc_sent[doc] = copy.deepcopy(sent_word)\n",
    "\n",
    "        clust_tags[clust] = copy.deepcopy(doc_sent)\n",
    "\n",
    "    return clust_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_clust_tags = vectorize_tags_across_clusters(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {'and': set(['OT']), 'humans': set(['NN']), 'sales': set(['NN']), 'topic': set(['NN']), 'put': set(['VB']), 'britain': set(['NN']), 'in': set(['OT']), 'subject': set(['NN']), 'beef': set(['NN']), 'to': set(['OT']), 'crimp': set(['NN']), 'has': set(['VB']), 'be': set(['VB']), 'that': set(['OT']), 'domestic': set(['JJ']), 'pubs': set(['NN']), 'press': set(['NN']), 'a': set(['OT']), 'made': set(['VB']), 'concern': set(['NN']), 'of': set(['OT']), 'disease': set(['NN']), 'p': set(['NN']), 'transmitted': set(['VB']), 'can': set(['OT']), 'serious': set(['JJ']), 'the': set(['OT'])}) \n",
      "\n",
      "\n",
      "defaultdict(<type 'set'>, {'and': [False, False, False, False], 'humans': [True, False, False, False], 'sales': [True, False, False, False], 'topic': [True, False, False, False], 'put': [False, True, False, False], 'britain': [True, False, False, False], 'in': [False, False, False, False], 'subject': [True, False, False, False], 'beef': [True, False, False, False], 'to': [False, False, False, False], 'crimp': [True, False, False, False], 'has': [False, True, False, False], 'be': [False, True, False, False], 'that': [False, False, False, False], 'domestic': [False, False, True, False], 'pubs': [True, False, False, False], 'press': [True, False, False, False], 'a': [False, False, False, False], 'made': [False, True, False, False], 'concern': [True, False, False, False], 'of': [False, False, False, False], 'disease': [True, False, False, False], 'p': [True, False, False, False], 'transmitted': [False, True, False, False], 'can': [False, False, False, False], 'serious': [False, False, True, False], 'the': [False, False, False, False]})\n"
     ]
    }
   ],
   "source": [
    "print old_cpy['mad cow disease']['LA060490-0083'][2],'\\n\\n'\n",
    "print new_clust_tags['mad cow disease']['LA060490-0083'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 5 : A binary value equals one iff the output of the named entity classifier from CoreNLP is not empty  (Named Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_ners(data_root_dir,annotation_file):\n",
    "    '''Perform Named Entity Recognition on all sentences in all docs in all clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_ner_cnt = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                \n",
    "                \n",
    "                ners = ner_tagger.tag(tok_sent)\n",
    "                cnt = 0\n",
    "                for ner in ners:\n",
    "                    if ner[1] != 'O':\n",
    "                        cnt += 1\n",
    "                sent_ner_cnt[s_id] = cnt\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_ner_cnt)\n",
    "            \n",
    "            print 'FINISHED NER ON ', file_name\n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED NER ON  FBIS4-27602\n",
      "FINISHED NER ON  AP880601-0040\n",
      "FINISHED NER ON  FT931-341\n",
      "FINISHED NER ON  AP901203-0166\n",
      "FINISHED NER ON  LA043089-0197\n",
      "FINISHED NER ON  AP901013-0046\n",
      "FINISHED NER ON  AP900625-0160\n",
      "FINISHED NER ON  AP900703-0040\n",
      "FINISHED NER ON  WSJ910607-0063\n",
      "FINISHED NER ON  LA080790-0111\n",
      "FINISHED NER ON  WSJ911224-0085\n",
      "FINISHED NER ON  LA050889-0075\n",
      "FINISHED NER ON  AP881017-0235\n",
      "FINISHED NER ON  WSJ900418-0193\n",
      "FINISHED NER ON  AP880705-0018\n",
      "FINISHED NER ON  AP881210-0115\n",
      "FINISHED NER ON  FT944-18184\n",
      "FINISHED NER ON  AP890529-0030\n",
      "FINISHED NER ON  AP900601-0040\n",
      "FINISHED NER ON  AP901130-0060\n",
      "FINISHED NER ON  LA100789-0007\n",
      "FINISHED NER ON  WSJ880617-0024\n",
      "FINISHED NER ON  FT942-11114\n",
      "FINISHED NER ON  FBIS3-51875\n",
      "FINISHED NER ON  AP880902-0062\n",
      "FINISHED NER ON  AP881206-0114\n",
      "FINISHED NER ON  AP900829-0120\n",
      "FINISHED NER ON  AP901010-0036\n",
      "FINISHED NER ON  FBIS4-4674\n",
      "FINISHED NER ON  LA103089-0070\n",
      "FINISHED NER ON  AP880927-0089\n",
      "FINISHED NER ON  LA101289-0194\n",
      "FINISHED NER ON  FT941-1547\n",
      "FINISHED NER ON  AP890313-0198\n",
      "FINISHED NER ON  FT934-5781\n",
      "FINISHED NER ON  AP900215-0031\n",
      "FINISHED NER ON  LA042790-0205\n",
      "FINISHED NER ON  AP880927-0117\n",
      "FINISHED NER ON  AP880928-0054\n",
      "FINISHED NER ON  AP880928-0146\n",
      "FINISHED NER ON  AP881009-0072\n",
      "FINISHED NER ON  AP891006-0029\n",
      "FINISHED NER ON  LA021689-0227\n",
      "FINISHED NER ON  LA030789-0047\n",
      "FINISHED NER ON  LA061589-0143\n",
      "FINISHED NER ON  LA092490-0095\n",
      "FINISHED NER ON  SJMN91-06084228\n",
      "FINISHED NER ON  SJMN91-06182091\n",
      "FINISHED NER ON  WSJ900615-0131\n",
      "FINISHED NER ON  WSJ870908-0047\n",
      "FINISHED NER ON  LA021090-0005\n",
      "FINISHED NER ON  WSJ910208-0130\n",
      "FINISHED NER ON  WSJ911121-0136\n",
      "FINISHED NER ON  WSJ911212-0080\n",
      "FINISHED NER ON  WSJ920114-0145\n",
      "FINISHED NER ON  WSJ920211-0036\n",
      "FINISHED NER ON  AP880318-0051\n",
      "FINISHED NER ON  SJMN91-06189077\n",
      "FINISHED NER ON  SJMN91-06142126\n",
      "FINISHED NER ON  AP890228-0019\n",
      "FINISHED NER ON  FT923-5089\n",
      "FINISHED NER ON  FT923-5267\n",
      "FINISHED NER ON  FT923-5797\n",
      "FINISHED NER ON  FT923-5835\n",
      "FINISHED NER ON  FT923-6038\n",
      "FINISHED NER ON  FT923-6110\n",
      "FINISHED NER ON  FT923-6455\n",
      "FINISHED NER ON  SJMN91-06143070\n",
      "FINISHED NER ON  FT933-10881\n",
      "FINISHED NER ON  FT933-2760\n",
      "FINISHED NER ON  FT934-10911\n",
      "FINISHED NER ON  FT934-13350\n",
      "FINISHED NER ON  FT934-8628\n",
      "FINISHED NER ON  FT934-8748\n",
      "FINISHED NER ON  FT934-9116\n",
      "FINISHED NER ON  FT941-1750\n",
      "FINISHED NER ON  WSJ910304-0002\n",
      "FINISHED NER ON  FT943-4951\n",
      "FINISHED NER ON  FT943-5628\n",
      "FINISHED NER ON  AP880926-0203\n",
      "FINISHED NER ON  AP880409-0015\n",
      "FINISHED NER ON  LA101690-0040\n",
      "FINISHED NER ON  AP880913-0204\n",
      "FINISHED NER ON  AP890307-0150\n",
      "FINISHED NER ON  WSJ910628-0109\n",
      "FINISHED NER ON  AP900419-0121\n",
      "FINISHED NER ON  LA042190-0060\n",
      "FINISHED NER ON  LA051190-0185\n",
      "FINISHED NER ON  FT943-12341\n",
      "FINISHED NER ON  AP890117-0132\n",
      "FINISHED NER ON  AP890322-0010\n",
      "FINISHED NER ON  LA092189-0123\n",
      "FINISHED NER ON  AP890302-0063\n",
      "FINISHED NER ON  AP900629-0260\n",
      "FINISHED NER ON  AP901031-0024\n",
      "FINISHED NER ON  LA120290-0163\n",
      "FINISHED NER ON  AP881216-0017\n",
      "FINISHED NER ON  AP881222-0119\n",
      "FINISHED NER ON  AP880520-0264\n",
      "FINISHED NER ON  FT911-2650\n",
      "FINISHED NER ON  FT922-8860\n",
      "FINISHED NER ON  FT932-12322\n",
      "FINISHED NER ON  FBIS3-11919\n",
      "FINISHED NER ON  AP891028-0022\n",
      "FINISHED NER ON  SJMN91-06301029\n",
      "FINISHED NER ON  LA110490-0184\n",
      "FINISHED NER ON  AP890511-0126\n",
      "FINISHED NER ON  AP890722-0081\n",
      "FINISHED NER ON  SJMN91-06276078\n",
      "FINISHED NER ON  FBIS3-41\n",
      "FINISHED NER ON  AP880811-0299\n",
      "FINISHED NER ON  LA103089-0043\n",
      "FINISHED NER ON  LA012590-0174\n",
      "FINISHED NER ON  LA080189-0042\n",
      "FINISHED NER ON  AP881018-0136\n",
      "FINISHED NER ON  WSJ910710-0123\n",
      "FINISHED NER ON  LA081890-0039\n",
      "FINISHED NER ON  AP890805-0126\n",
      "FINISHED NER ON  FT911-3463\n",
      "FINISHED NER ON  AP900322-0192\n",
      "FINISHED NER ON  WSJ910304-0005\n",
      "FINISHED NER ON  AP901029-0035\n",
      "FINISHED NER ON  FBIS3-23360\n",
      "FINISHED NER ON  LA040689-0056\n",
      "FINISHED NER ON  WSJ880923-0163\n",
      "FINISHED NER ON  FT911-5176\n",
      "FINISHED NER ON  AP890907-0221\n",
      "FINISHED NER ON  AP881222-0089\n",
      "FINISHED NER ON  AP890111-0227\n",
      "FINISHED NER ON  AP891201-0100\n",
      "FINISHED NER ON  AP900323-0036\n",
      "FINISHED NER ON  AP900416-0188\n",
      "FINISHED NER ON  AP900910-0020\n",
      "FINISHED NER ON "
     ]
    }
   ],
   "source": [
    "clust_ners = extract_ners(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 6 : A binary value denotes if a word in Number  (Number)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_digit_cnt(data_root_dir,annotation_file):\n",
    "    '''Count the number of digits in a sentence'''\n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            dig_cnt = 0\n",
    "            for s_id,tok_sent in enumerate(sent_tokens):                                    \n",
    "                for tok in tok_sent:\n",
    "                    if tok.isdigit():\n",
    "                        dig_cnt += 1\n",
    "                sent_dig_cnt[s_id] = dig_cnt\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_digs = extract_digit_cnt(data_root_dir,annotation_file)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_digs = extract_digit_cnt(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "clust_digs['mad cow disease']['LA060490-0083'][29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 22 : The number of digits, divided by the sentence length(Number ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_dig_ratio = extract_digit_cnt(data_root_dir,annotation_file,'R')\n",
    "print 'done'\n",
    "clust_dig_ratio['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 23 : The number of stop words, divided by the sentence length(Stop word ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_word_ratio(data_root_dir,annotation_file):\n",
    "    '''Compute the stop word ratio for all sentences'''\n",
    "    '''stop word ratio == no of stop words in sent / len(sent) '''\n",
    "    \n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):      \n",
    "                stop_cnt = 0\n",
    "                for tok in tok_sent:\n",
    "                    if tok.lower() in english_stopwords:\n",
    "                        stop_cnt += 1\n",
    "                sent_dig_cnt[s_id] = float(stop_cnt)/len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_digs = stop_word_ratio(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "clust_digs['mad cow disease']['LA060490-0083'][18]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
